
<!DOCTYPE html>
<html>
  <head>
    
<meta charset="utf-8" >

<title>What are the Most Important Statistical Ideas of the Past 50 Years? | Dwoooh</title>
<meta name="description" content="">

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.min.css">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://https://davidliu0901.github.io//favicon.ico?v=1665123304460">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link rel="stylesheet" href="https://https://davidliu0901.github.io//styles/main.css">



<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>



  </head>
  <body>
    <div id="app" class="main">
      <div class="site-header-container">
  <div class="site-header">
    <div class="left">
      <a href="https://https://davidliu0901.github.io/">
        <img class="avatar" src="https://https://davidliu0901.github.io//images/avatar.png?v=1665123304460" alt="" width="32px" height="32px">
      </a>
      <a href="https://https://davidliu0901.github.io/">
        <h1 class="site-title">Dwoooh</h1>
      </a>
    </div>
    <div class="right">
      <transition name="fade">
        <i class="icon" :class="{ 'icon-close-outline': menuVisible, 'icon-menu-outline': !menuVisible }" @click="menuVisible = !menuVisible"></i>
      </transition>
    </div>
  </div>
</div>

<transition name="fade">
  <div class="menu-container" style="display: none;" v-show="menuVisible">
    <div class="menu-list">
      
        
          <a href="/" class="menu purple-link">
            È¶ñÈ°µ
          </a>
        
      
        
          <a href="/archives" class="menu purple-link">
            ÂΩíÊ°£
          </a>
        
      
        
          <a href="/tags" class="menu purple-link">
            Ê†áÁ≠æ
          </a>
        
      
        
          <a href="/post/about" class="menu purple-link">
            ÂÖ≥‰∫é
          </a>
        
      
    </div>
  </div>
</transition>


      <div class="content-container">
        <div class="post-detail">
          
          <h2 class="post-title">What are the Most Important Statistical Ideas of the Past 50 Years?</h2>
          <div class="post-info post-detail-info">
            <span><i class="icon-calendar-outline"></i> 2021-07-13</span>
            
          </div>
          <div class="post-content">
            <!-- more -->
<p>Andrew Gelmana and Aki Vehtarib</p>
<p>https://www.tandfonline.com/doi/pdf/10.1080/01621459.2021.1938081?needAccess=true</p>
<p>1.1. Counterfactual Causal Inference<br>
The purpose of the aforementioned methods is to define and estimate the effect of some specified treatment or exposure, adjusting for biases arising from imbalance, selection, and measurement errors.</p>
<p>1.2. Bootstrapping and Simulation-Based Inference<br>
In permutation testing, resampled datasets are generated by breaking the (possible) dependency between the predictors and target by randomly shuffling the target values.</p>
<p>1.3. Overparameterized Models and Regularizationüòç<br>
Regularization can be implemented as a penalty function on the parameters or on the predicted curve (Good and Gaskins 1971).</p>
<p>1.4. Bayesian Multilevel Models<br>
generalize data to new problems (meta-analysis). Multilevel models formalized ‚Äúempirical Bayes‚Äù techniques of estimating a prior distribution from data.</p>
<p>1.5. Generic Computation Algorithms<br>
The EM algorithm (Dempster, Laird, and Rubin 1977; Meng and van Dyk<br>
1997), Gibbs sampler (Geman and Geman 1984; Gelfand and Smith 1990), particle filters (Kitagawa 1993; Gordon, Salmond, and Smith 1993; Del Moral 1996), variational inference (Jordan et al. 1999), and expectation propagation (Minka 2001, Heskes et al. 2005) in different ways make use of the conditional independence structures of statistical models.</p>
<p>The Metropolis algorithm (Hastings, 1970) and hybrid or Hamiltonian Monte<br>
Carlo (Duane et al. 1987) were less directly motivated by statistical concerns‚Äîthese were methods that were originally developed to compute high-dimensional probability distributions in physics‚Äîbut they have become adapted to statistical computing in the same way that optimization algorithms were adopted in an earlier era to compute least squares and maximum likelihood<br>
estimates.</p>
<p>The method called approximate Bayesian computation, in which posterior inferences are obtained by simulating from the generative model instead of evaluating the likelihood function, can be useful if the analytic form of the likelihoodis intractable or very costly to compute (Rubin 1984; Tavar√©<br>
et al. 1997; Marin et al. 2012). Martin, Frazier, and Robert‚Äôs<br>
(2020)üòçüòç review the history of computational methods in Bayesian<br>
statistics.</p>
<p>1.6. Adaptive Decision Analysis<br>
error-rate control; false discovery rate analysis<br>
involve Bayesian optimization (Mockus 1974; Mockus 2012; Shahriari et al. 2015) and reinforcement learning(Sutton and Barto 2018),</p>
<p>1.7. Robust Inference<br>
The idea of robustness is central to modern statistics, and it is all about the idea that we can use models even when they have assumptions that are not true.<br>
Concerns of robustness are relevant for the densely parameterized models<br>
that are characteristic of much of modern statistics, and this has implications for model evaluation more generally (Navarro<br>
2019).<br>
There is a connection between robustness of a statistical<br>
method to model misspecification, and a workflow involving<br>
model checking and model improvement (Box 1980).</p>
<p>1.8. Exploratory Data Analysis<br>
graphical visualization of data.</p>
<p>We suspect it is the value of the methods that has made the names sound appealing, rather than the reverse.</p>
<p>Meta-algorithms‚Äîworkflows that make use of existing models and inferential procedures. (Boosting)</p>
<p>Pyro!!!!!!!!!!!!!!!!!!!!!üòçüòçüòçüòçüòç</p>
<p>Highly parameterized machine learning methods can be framed as Bayesian hierarchical models, with regularizing penalty functions corresponding to hyperpriors, and unsupervised learning models can be framed as mixture models with unknown group memberships.</p>
<p>interpretable machine learning (Murdoch et al. 2019; Molnar 2020).</p>

          </div>
        </div>

        
          <div class="next-post">
            <a class="purple-link" href="https://https://davidliu0901.github.io/post/immunoinformatics-predicting-peptide-mhc-binding/">
              <h3 class="post-title">
                ‰∏ã‰∏ÄÁØáÔºöImmunoinformatics: Predicting Peptide‚ÄìMHC Binding
              </h3>
            </a>
          </div>
          
      </div>

      

      <div class="site-footer">
  <div class="slogan"></div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
   | <a class="rss" href="https://https://davidliu0901.github.io//atom.xml" target="_blank">RSS</a>
</div>


    </div>
    <script type="application/javascript">

hljs.initHighlightingOnLoad()

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>




  </body>
</html>
